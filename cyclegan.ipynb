{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as T\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport os\nfrom glob import glob\n\n# import torch_xla.core.xla_model as xm\n# import torch_xla.distributed.parallel_loader as pl\n# import torch_xla.distributed.xla_multiprocessing as xmp\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"execution":{"iopub.status.busy":"2023-07-07T10:48:22.101503Z","iopub.execute_input":"2023-07-07T10:48:22.102489Z","iopub.status.idle":"2023-07-07T10:48:22.108876Z","shell.execute_reply.started":"2023-07-07T10:48:22.102450Z","shell.execute_reply":"2023-07-07T10:48:22.107823Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, monet_path, photo_path, transform=None):\n        super().__init__()\n        self.monet_path = monet_path\n        self.photo_path = photo_path\n        self.monet_files = glob(self.monet_path+'/*')\n        self.photo_files = glob(self.photo_path+'/*')\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        idx2 = np.random.randint(len(self.photo_files))\n        x1 = Image.open(self.monet_files[idx])\n        x2 = Image.open(self.photo_files[idx2])\n        if self.transform is not None:\n            x1 = self.transform(x1)\n            x2 = self.transform(x2)\n        return x1, x2\n    \n    def __len__(self):\n        return len(self.monet_files)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T10:48:23.393954Z","iopub.execute_input":"2023-07-07T10:48:23.394323Z","iopub.status.idle":"2023-07-07T10:48:23.403111Z","shell.execute_reply.started":"2023-07-07T10:48:23.394292Z","shell.execute_reply":"2023-07-07T10:48:23.402147Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"feature_transform = T.Compose([T.Resize((256,256)),T.ToTensor(),T.RandomVerticalFlip()])\n\nds = ImageDataset('/kaggle/input/gan-getting-started/monet_jpg','/kaggle/input/gan-getting-started/photo_jpg', transform=feature_transform)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T10:48:24.753741Z","iopub.execute_input":"2023-07-07T10:48:24.754089Z","iopub.status.idle":"2023-07-07T10:48:25.091698Z","shell.execute_reply.started":"2023-07-07T10:48:24.754059Z","shell.execute_reply":"2023-07-07T10:48:25.090735Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!export XLA_USE_BF16=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class G_up_Block(nn.Module):\n    def __init__(self, in_channels, out_channels, k, stride=1, padding=0):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, k, stride=stride ,padding=padding),\n            nn.InstanceNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n    def forward(self, x):\n        x = self.layers(x)\n        return x\n\nclass Residual_Block(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.layers = nn.Sequential(\n            G_up_Block(channels, channels, 3, 1, 1),\n            G_up_Block(channels, channels, 3, 1, 1)\n        )\n    def forward(self, x):\n        return x + self.layers(x)\n\nclass G_down_Block(nn.Module):\n    def __init__(self, in_channels, out_channels, k, stride=1, padding=0, output_padding=1):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, k, stride, padding, output_padding),\n            nn.InstanceNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n    def forward(self, x):\n        x = self.layers(x)\n        return x\n\nclass Generator(nn.Module):\n    def __init__(self, num_residual_blocks):\n        super().__init__()\n        self.num_residual_blocks = num_residual_blocks\n        self.up = nn.Sequential(\n            G_up_Block(3,64,7,1,3),\n            G_up_Block(64,128,3,2,1),\n            G_up_Block(128,256,3,2,1)\n        )\n        self.residual = nn.Sequential(*[Residual_Block(256) for _ in range(self.num_residual_blocks)])\n        self.down = nn.Sequential(\n            G_down_Block(256,128,3,2,1,1),\n            G_down_Block(128,64,3,2,1,1),\n            G_up_Block(64,3,7,1,3)\n        )\n    def forward(self, x):\n        x = self.up(x)\n        x = self.residual(x)\n        x = self.down(x)\n        return x\n\nclass D_Block(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=2, padding=1, norm=True):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 4, stride, padding),\n            nn.InstanceNorm2d(out_channels) if norm else nn.Identity(),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n    def forward(self, x):\n        x = self.layers(x)\n        return x\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Sequential(\n            D_Block(3,64,2,1,norm=False),\n            D_Block(64,128,2,1),\n            D_Block(128,256,2,1),\n            D_Block(256,512,1,1),\n            D_Block(512,1,1,1),\n        )\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.sigmoid(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-07-07T10:48:28.132756Z","iopub.execute_input":"2023-07-07T10:48:28.133105Z","iopub.status.idle":"2023-07-07T10:48:28.153910Z","shell.execute_reply.started":"2023-07-07T10:48:28.133074Z","shell.execute_reply":"2023-07-07T10:48:28.153002Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def train(loader, monet_G, photo_G, monet_D, photo_D, L1, mse, G_optim, D_optim, device, lambda_cycle=10):\n    total_G_loss = 0.0 \n    total_D_loss = 0.0 \n    running_D_loss = 0.0  \n    running_G_loss = 0.0\n    print_interval = 100\n    for batch_id, (monet, photo) in enumerate(loader):        \n        monet = monet.to(device)\n        photo = photo.to(device)\n\n        fake_monet = monet_G(photo)\n        fake_photo = photo_G(monet)\n        \n        #train discriminator\n        critic_monet_real = monet_D(monet)\n        critic_monet_fake = monet_D(fake_monet.detach())\n        critic_monet_real_loss = mse(critic_monet_real, torch.ones_like(critic_monet_real))\n        critic_monet_fake_loss = mse(critic_monet_fake, torch.zeros_like(critic_monet_fake))\n        critic_monet_loss = critic_monet_real_loss + critic_monet_fake_loss\n        \n        critic_photo_real = photo_D(photo)\n        critic_photo_fake = photo_D(fake_photo.detach())\n        critic_photo_real_loss = mse(critic_photo_real, torch.ones_like(critic_photo_real))\n        critic_photo_fake_loss = mse(critic_photo_fake, torch.zeros_like(critic_photo_fake))\n        critic_photo_loss = critic_photo_real_loss + critic_photo_fake_loss\n        \n        D_loss = critic_monet_loss + critic_photo_loss\n        \n        D_optim.zero_grad()\n        D_loss.backward()\n        #xm.optimizer_step(D_optim)\n        D_optim.step()\n        \n        running_D_loss += D_loss.item()\n        total_D_loss += D_loss.item()\n        \n        #train generator\n        critic_monet_fake = monet_D(fake_monet)\n        critic_photo_fake = photo_D(fake_photo)\n        \n        #adverserial loss\n        gen_monet_loss = mse(critic_monet_fake, torch.ones_like(critic_monet_fake))\n        gen_photo_loss = mse(critic_photo_fake, torch.ones_like(critic_photo_fake))\n        \n        #cycle loss\n        cycle_monet = monet_G(fake_photo)\n        cycle_photo = photo_G(fake_monet)\n        cycle_monet_loss = L1(monet, cycle_monet)\n        cycle_photo_loss = L1(photo, cycle_photo)\n        \n        G_loss = (gen_monet_loss + gen_photo_loss) + (cycle_monet_loss + cycle_photo_loss)*lambda_cycle #+ (identity_monet_loss + identity_photo_loss)\n\n        G_optim.zero_grad()\n        G_loss.backward(retain_graph=True)\n        #xm.optimizer_step(G_optim)\n        G_optim.step()\n        \n        running_G_loss += G_loss.item()\n        total_G_loss += G_loss.item()\n        \n        if batch_id % print_interval == print_interval-1:\n            print(f'[{step:5d}] G_loss: {running_G_loss / print_interval:.3f}, D_loss:{running_G_loss / print_interval:.3f}')\n            running_G_loss = 0.0\n            running_D_loss = 0.0\n        \n        return total_G_loss/len(loader), total_D_loss/len(loader)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T10:53:30.835793Z","iopub.execute_input":"2023-07-07T10:53:30.836196Z","iopub.status.idle":"2023-07-07T10:53:30.852301Z","shell.execute_reply.started":"2023-07-07T10:53:30.836153Z","shell.execute_reply":"2023-07-07T10:53:30.851197Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"G_losses, D_losses = [], []","metadata":{"execution":{"iopub.status.busy":"2023-07-07T10:53:31.992868Z","iopub.execute_input":"2023-07-07T10:53:31.993260Z","iopub.status.idle":"2023-07-07T10:53:31.997865Z","shell.execute_reply.started":"2023-07-07T10:53:31.993229Z","shell.execute_reply":"2023-07-07T10:53:31.996746Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"ds.__len__()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T10:54:35.363113Z","iopub.execute_input":"2023-07-07T10:54:35.363574Z","iopub.status.idle":"2023-07-07T10:54:35.370957Z","shell.execute_reply.started":"2023-07-07T10:54:35.363511Z","shell.execute_reply":"2023-07-07T10:54:35.369954Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"300"},"metadata":{}}]},{"cell_type":"code","source":"loader = DataLoader(ds, batch_size=1, num_workers=2, drop_last=True, shuffle=True)\nprint(len(loader))","metadata":{"execution":{"iopub.status.busy":"2023-07-07T10:54:19.698522Z","iopub.execute_input":"2023-07-07T10:54:19.698931Z","iopub.status.idle":"2023-07-07T10:54:19.704894Z","shell.execute_reply.started":"2023-07-07T10:54:19.698900Z","shell.execute_reply":"2023-07-07T10:54:19.703821Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"300\n","output_type":"stream"}]},{"cell_type":"code","source":"def _run():\n    global G_losses\n    global D_losses\n    \n    batch_size = 32\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    #device = xm.xla_device()\n    \n    #train_sampler = torch.utils.data.distributed.DistributedSampler(ds,num_replicas=xm.xrt_world_size(),rank=xm.get_ordinal(),shuffle=True)\n    #loader = DataLoader(ds, batch_size=batch_size, num_workers=4, drop_last=True, sampler=train_sampler)\n    \n    loader = DataLoader(ds, batch_size=batch_size, num_workers=2, drop_last=True, shuffle=True)\n    monet_G = Generator(9).to(device)\n    photo_G = Generator(9).to(device)\n    monet_D = Discriminator().to(device)\n    photo_D = Discriminator().to(device)\n    \n    #lr = 0.4 * 1e-3 * xm.xrt_world_size()\n    lr = 1e-3\n    betas = (0.5,0.999)\n\n    G_optim = optim.Adam(list(monet_G.parameters())+list(photo_G.parameters()), lr=lr, betas=betas)\n    D_optim = optim.Adam(list(monet_D.parameters())+list(photo_D.parameters()), lr=lr, betas=betas)\n    \n    L1 = nn.L1Loss()\n    mse = nn.MSELoss()\n    \n    EPOCHS = 10\n    \n    for epoch in range(EPOCHS):\n        #para_loader = pl.ParallelLoader(loader, [device])\n        print(f'[{epoch+1:2d}/{EPOCHS}]')\n        #g_loss, d_loss = train(para_loader.per_device_loader(device), monet_G, photo_G, monet_D, photo_D, L1, mse, D_optim, G_optim)\n        g_loss, d_loss = train(loader, monet_G, photo_G, monet_D, photo_D, L1, mse, G_optim, D_optim, device)\n        G_losses.append(g_loss)\n        D_losses.append(d_loss)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T10:53:32.798108Z","iopub.execute_input":"2023-07-07T10:53:32.798474Z","iopub.status.idle":"2023-07-07T10:53:32.808515Z","shell.execute_reply.started":"2023-07-07T10:53:32.798442Z","shell.execute_reply":"2023-07-07T10:53:32.807366Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"_run()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T10:53:33.969025Z","iopub.execute_input":"2023-07-07T10:53:33.969402Z","iopub.status.idle":"2023-07-07T10:53:36.591889Z","shell.execute_reply.started":"2023-07-07T10:53:33.969369Z","shell.execute_reply":"2023-07-07T10:53:36.590312Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"[ 1/10]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[30], line 34\u001b[0m, in \u001b[0;36m_run\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#g_loss, d_loss = train(para_loader.per_device_loader(device), monet_G, photo_G, monet_D, photo_D, L1, mse, D_optim, G_optim)\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m g_loss, d_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonet_G\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphoto_G\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonet_D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphoto_D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG_optim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD_optim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m G_losses\u001b[38;5;241m.\u001b[39mappend(g_loss)\n\u001b[1;32m     36\u001b[0m D_losses\u001b[38;5;241m.\u001b[39mappend(d_loss)\n","Cell \u001b[0;32mIn[28], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(loader, monet_G, photo_G, monet_D, photo_D, L1, mse, G_optim, D_optim, device, lambda_cycle)\u001b[0m\n\u001b[1;32m      8\u001b[0m monet \u001b[38;5;241m=\u001b[39m monet\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m photo \u001b[38;5;241m=\u001b[39m photo\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 11\u001b[0m fake_monet \u001b[38;5;241m=\u001b[39m \u001b[43mmonet_G\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphoto\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m fake_photo \u001b[38;5;241m=\u001b[39m photo_G(monet)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#train discriminator\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[5], line 53\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup(x)\n\u001b[1;32m     52\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual(x)\n\u001b[0;32m---> 53\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[5], line 32\u001b[0m, in \u001b[0;36mG_down_Block.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:74\u001b[0m, in \u001b[0;36m_InstanceNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_no_batch_dim():\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_no_batch_input(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_instance_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:34\u001b[0m, in \u001b[0;36m_InstanceNorm._apply_instance_norm\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply_instance_norm\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstance_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2495\u001b[0m, in \u001b[0;36minstance_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, use_input_stats, momentum, eps)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_input_stats:\n\u001b[1;32m   2494\u001b[0m     _verify_spatial_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstance_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2496\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_input_stats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 15.90 GiB total capacity; 13.99 GiB already allocated; 137.75 MiB free; 14.89 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 15.90 GiB total capacity; 13.99 GiB already allocated; 137.75 MiB free; 14.89 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"def _mp_fn(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = _run()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FLAGS = {}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}